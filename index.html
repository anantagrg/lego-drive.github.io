<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LeGo-Drive</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4pZlCV4AAAAJ&hl=en">Pranjal Paul*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://anantagrg.github.io/">Anant Garg*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Tushar Chaudhary</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/akslab">Arun Singh</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://robotics.iiit.ac.in/faculty_mkrishna/">K. Madhav Krishna</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>International Institute of Information Technology - Hyderabad</span>
            <span class="author-block"><sup>2</sup>University of Tartu, Estonia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1dYEh2VItCpv45ILI-hOjXenw9oU8unjI/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.20116"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/56etmpWrnFg "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <img id="teaser-image" src="./static/images/lego-drive-teaser.png" alt="Teaser Image" height="100%">
      </div>
      <p>
        The proposed method, LeGo-Drive estimates a goal location queried with a navigation instruction- “Park near the bus stop on
        the front-left” on a single front-facing camera image and coupled it with a differentiable optimizer-based planner that jointly optimizes
        the trajectory and the goal location. (Left) The proposed architecture is shown along with the gradient flow for joint end-to-end training.
        (Right-Top) Goal improvement from initial estimation in Green to improved location in Red. (Right-Bottom) Trajectory output in Red
        leading to the improved goal location, compared with the trajectory generated by baseline in Green.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing Vision-Language Models (VLMs) produce long-term trajectory waypoints or directly control actions
            based on their perception input and language prompt. However,
            these VLMs are not explicitly aware of the constraints imposed
            by the scene or kinematics of the vehicle. As a result, the
            generated trajectories or control inputs are likely to be unsafe
            and/or infeasible. In this paper, we introduce LeGo-Drive † ,
            which aims to address these issues. Our key idea is to use
            the VLM to just predict a goal location based on the given
            language command and perception input, which is then fed to
            a downstream differentiable trajectory optimizer with learnable
            components. We train the VLM and the trajectory optimizer
            in an end-to-end fashion using a loss function that captures the
            ego-vehicle’s ability to reach the predicted goal while satisfying
            safety and kinematic constraints. The gradients during the
            back-propagation flow through the optimization layer and make
            the VLM aware of the planner’s capabilities, making more
            feasible goal predictions. We compare our end-to-end approach
            with a decoupled framework where the planner is just used
            at the inference time to drive to the VLM-predicted goal
            location and report a goal reaching Success Rate of 81%.
            We demonstrate the versatility of LeGo-Drive † across various
            driving scenarios and navigation commands, highlighting its
            potential for practical deployment in autonomous vehicles.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/eOYAq2cz1Pk"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Park-In</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/park-in_1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <h2 class="title is-3">Park-Out</h2>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/park-out_1.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Right Turn</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/right-turn_1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <h2 class="title is-3">Cross Left Turn</h2>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/cross-left-turn_1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-3">Visual Question Answering</h2>
        <div class="content">
          <p>
            Given the front-view RGB image, we employ GPT-4V [29]
            to determine the best course of action from a range of potential
            driving maneuvers. GPT-4V accurately identifies an obstruction
            ahead and recommends switching to the left lane to continue
            moving forward. With this recommended command, our pipeline
            is able to predict a collision-free goal point and an optimized
            trajectory.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vqa-truck_1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered">
          <!-- <h2 class="title is-3">Compound Command</h2> -->
          <div class="column content">
            <h2 class="title is-3">Compound Command</h2>
            <p>
              Compound commands such as <i>"Turn right and stop by the food stall on the right"</i> is broken into atomic commands: 
              <i>Turn right</i> and <i>Stop by the food stall on the right</i>.
              These atomic commands are then executed sequentially.
              <br>
              <br>
              <br>
              <br>
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/compund_command_1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <!--/ Matting. -->
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
